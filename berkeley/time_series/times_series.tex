\documentclass[11pt]{article}
\usepackage{hyperref} 
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage[margin=1in]{geometry}

\parindent0px

\emergencystretch=0pt
\pretolerance=150
\tolerance=10000
\hbadness=10000
\hfuzz=0pt

\title{Time Series Notes}
\author{Nathan Ueda}
\date{\today} 

\begin{document}
\maketitle 
\pagebreak
\tableofcontents 
\pagebreak

\section{Time Series Basics}
% \subsection{The History of Probability}

\textbf{What is a time series?}
\begin{itemize}
    \item A \textit{time series} is a sequence of observations of the same variable indexed in 
    time order (i.e. monthly stock returns). 
    \item Defining $x_t$ as a r.v., a time series may be written 
    \[ \{x_1, x_2, \ldots, x_T\} \text{ or }  \{x_t\}, \, t = 1, 2, \ldots, T \]
\end{itemize}

\textbf{Modeling time series data}
\begin{itemize}
    \item The models impose structure, so when dealing with model selection, it is important 
    to evaluate to see if the model captures the features you believe to be present in the data.
\end{itemize}

\section{Stationarity and Ergodicity}

\subsection{Basics and Importance}
\begin{itemize}
    \item Time series are typically not i.i.d. (i.e. if GNP is unusually high today, GNP will 
    likely be unusually high tomorrow). So instead, we need different desirable properties for 
    time series data. Two of the most important of these properties are stationarity and 
    ergodicity. 
    \item Stationarity is a property for time series with some time-invariant behavior.
    \item Ergodicity is a property for times series that expresses the idea that the effect of 
    the past on the future eventually dies out.
    \item Time series with these properties are easier to estimate.
    \item Time series that are nonstationarity and nonergodic require a different set of
    techniques.
    \item Under the assumption of i.i.d r.v.s, we had LLN and CLT. For r.v.s that are not 
    i.i.d. and are instead autocorrelated, stationarity and ergodicity are similar results.
    \item In practice, by saying the time series is stationary, it is typically implied it is both
    stationary and ergodic. 
    \item If a times series is stationary, it does not imply ergodicity; however, a time series 
    that is ergodic is always stationary.
    \item Takeaway: Ensure your time series is stationary and ergodic. 
\end{itemize}

\subsection{Strongly Stationarity and Weakly Stationarity}
\begin{itemize}
    \item A process $\{x_t\}$ is \textit{strongly stationary} or \textit{strictly stationary}
    if all aspects of its behavior are unchanged by shifts in time. More formally, it is
    defined as the requirement that for every $m$ and $n$, the distributions of $\{x_1, \ldots,
    x_n\}$ and $\{x_{1+m}, \ldots, x_{n+m}\}$ are the same, that is, the joint probability 
    distribution of a sequence of $n$ observations does not depend on their time origin.
   \item A process $\{x_t\}$ is \textit{weakly stationary} or \textit{covariance stationary} if 
   its mean, variance, and covariance are unchanged by time shifts. More formally, it is 
   defined as weakly stationary if 
   \begin{itemize}
    \item First moment is a finite constant: $E(x_t) = \mu$
    \item Second moment is a finite constant: $Var(x_t) = \sigma^2$
    \item $\text{Cov}(x_t, x_s) - \gamma(|t-s|)$
   \end{itemize}
   In other words, the mean and variance do not change with time and the covariance between 
   two observations depends on the time distance between them (aka having same number of 
   observations), not the specific points. 
   \item Strong stationarity does not imply weak stationarity and weak stationarity does not 
   imply strong stationarity.    
   \item Stationarity is important as a stationary process can be modeled with relatively few 
   parameters.
\end{itemize}

\subsection{Testing for Stationarity}
\begin{itemize}
    \item When a times series is observed, a natural question is whether it appears to be 
    stationary. 
    \item \textbf{Time series plot}:
    Looking at a \textit{time series plot} (plot of the series in chronological 
    order) may be useful. If the time series is a stationary series, it should show some random 
    oscillation around some fixed level, a phenomenon called \textit{mean reversion}. If the 
    series wanders without returning repeatedly to some fixed level, then the series should not
    be modeled as a stationary process. 
    \begin{figure}[H] 
         \centering 
         \includegraphics[width=4in]{imgs/stationary_vs_nonstationary.png}
         \caption{Stationary vs. non-stationary time series}
     \end{figure}
     \item If all the roots of a characteristic polynomial for an AR process are greater than 1 
     in absolute value, then the AR process is stationary.
     \item Any finite-order $\text{MA}(q)$ process is stationary and ergodic.
\end{itemize}

\subsection{Ergodicity}
\begin{itemize}
    \item A stochastic process $\{x_t\}$ is ergodic if any two random variables sufficiently 
    far apart in time are essentially independent. In other words, $\{x_t\}$ is ergodic if 
    $x_t$ and $x_{t-j}$ are close to uncorrelated if $j$ is large enough.
\end{itemize}

\subsection{Law of Large Numbers and Central Limit Theorem for an Autocorrelated Process}
\begin{itemize}
    \item The \textit{ergodic theorem} is a LLM for an autocorrelated process that states, if 
    $\{x_t\}$ is stationary and ergodic, then 
    \[\bar{x}_T= \frac{1}{T} \sum_{t=1}^{T} x_t \rightarrow \text{E}(x_t) \text{ as } T 
    \rightarrow \infty\]
    \item If $\{x_t\}$ is stationary and ergodic, then the asymptotic distribution of the 
    sample mean is normal.
\end{itemize}

\section{White Noise}

\begin{figure}[H] 
    \centering 
    \includegraphics[width=4in]{imgs/white_noise.png}
    \caption{White noise}
\end{figure}

\begin{itemize}
    \item The building block for time series models is the \textit{white noise process}, 
    denoted $\epsilon_t$.
    \item In the least general case,
    \[ \epsilon_t \sim \text{i.i.d. } N(0, \sigma_{\epsilon}^{2})\]
    The assumption of i.i.d. have the following implications 
    \begin{enumerate}
        \item No predictability: Past values of a white noise process contain no information to predict future 
        values. Therefore, the best predictor is its mean, which is the same prediction without 
        observing past values. More formally, 
        \[E(\epsilon) = E(\epsilon_t | \epsilon_{t-1}, \epsilon_{t-2} \ldots) = 0\]
        \item No autocorrelation: Each observation is independent each other. More formally, 
        \[E(\epsilon_t \epsilon_{t-j}) = \text{Cov}(\epsilon_t \epsilon_{t-j})=0\]
        \item Conditional homoskedasticity: The conditional variance is a constant, and is 
        the same variance without observing past values. More formally, 
        \[\text{Var}(\epsilon_t) = \text{ Var}(\epsilon_t | \epsilon_{t-1}, \epsilon_{t-2} 
        \ldots) = \sigma_{\epsilon}^{2}\]
    \end{enumerate}
\end{itemize}

\section{Autocovariance and Autocorrelation}
\subsection{Autocovariance}
\begin{itemize}
    \item Autocovariance: Specifies the covariance between the value of a process at two times.
    \item Covariance: A nonstandardized measure to quantify the relationship between two 
    variables. Can take on any value. A positive value means the variables tend to move in the 
    same direction, a negative values means the variables tend to move in the opposite 
    direction, and a zero value means they are independent and don't move in relation to each 
    other.
    \item The \textit{autocovariance} of a series $x_t$ is defined as 
    \[\gamma_j = \text{Cov}(x_t, x_{t-j})\]
    \item $\gamma(h) = \gamma(-h)$ since what is important is the the space between the two 
    observations, rather than the exact observations themselves. 
    \item $\gamma_0 = \sigma^2$
\end{itemize}

\subsection{Autocorrelation}
\begin{itemize}
    \item Autocorrelation (serial correlation): The degree of correlation of the same variable 
    between different time intervals.
    \item Correlation: A standardized measure to quantify the relationship between two 
    variables. Can take on a value inclusively between -1 and 1.
    \item A time series with autocorrelation implies that, predictive power (i.e. knowing the
    price of a stock today helps forecast its price tomorrow).
    \item The \textit{autocorrelation} of a series $x_t$ is defined as 
    \[\rho_j = \frac{\gamma_j}{\gamma_0}\]
\end{itemize}

\subsection{Testing for autocorrelation}
\textbf{ACF Plots}
\begin{itemize}
    \item Show a correlation between a time series and lagged versions of itself. 
    \item The plot also includes test bounds used to test the null hypothesis that an 
    autocorrelation coefficient is 0. The null is rejected if the sample autocorrelatioon is
    outside the bounds, The usual level of the test is 0.05, so one can expect to see about 1
    out of 20 samples outside the bounds simply by chance. 

    \begin{figure}[H] 
        \centering 
        \includegraphics[width=5in]{imgs/acf_plots.png}
        \caption{Sample ACF plots of (a) one-month inflation rate (decays slowly, indicating 
        either nonstationarity or long-memory dependence) and (b) changes in the inflation rate
        (decays to 0 quickly, indicating that it is stationary).}
    \end{figure}

\end{itemize}

\textbf{Ljung-Box Test}
\begin{itemize}
    \item A \textit{simultaneous test} is one that tests whether a group of null hypotheses area
    all true versus the alternative that at least one of them is false.
    \item The \textit{Ljung-Box test} is a simultaneous test where the null is $\rho_1 = \rho_2
    = \hdots =\rho_K$ for some $K$. 
    \item If the Ljung-Box test rejects, then we conclude that one or more of $\rho_1, \rho_2, 
    \ldots, \rho_K$ is nonzero. 
\end{itemize}

\section{Lag and Difference Operators}
\begin{itemize}
    \item These operators are useful in describing some time series models.
    \item They allow us to simplify notation. 
\end{itemize}
\subsection{Lag Operators}
\begin{itemize}
    \item The lag operator moves the index back one unit in time.
    \item The lag operator $L$ is defined 
    \[ L x_t = x_{t-1}\]
    \[ L^j x_t = x_{t-j}\]
    \item Also commonly referred to as a backwards operator. 
    \item Given an $ARMA(p,q)$ model, we could simplify it as follows 
    \begin{align*}
        Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \hdots + \phi_p Y_{t-p} + \epsilon_t + \theta_1
        \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \hdots + \theta_q \epsilon_{t-q} \\ 
        Y_t - \phi_1 Y_{t-1} - \phi_2 Y_{t-2} - \hdots - \phi_p Y_{t-p} = \epsilon_t + \theta_1
        \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \hdots + \theta_q \epsilon_{t-q} \\ 
        Y_t - \phi_1 L Y_{t} - \phi_2 L^2 Y_{t} - \hdots - \phi_p L^p Y_{t} = \epsilon_t + 
        \theta_1 L \epsilon_{t} + \theta_2 L^2 \epsilon_{t} + \hdots + \theta_q L^q 
        \epsilon_{t} \\ 
        (1 - \phi_1 L - \phi_2 L^2 - \hdots - \phi_p L^p)Y_t = (1 + \theta_1 L + \theta_2 L^2 
        + \hdots + \theta_q L^q) \epsilon_t \\ 
    \end{align*}
\end{itemize}

\subsection{Difference Operators}
\begin{itemize}
    \item Defined as $\Delta = 1 - L$ so that 
    \[\Delta x_t = x_t - L x_t = x_t - x_{t-1}\]
    \item $\Delta^k$ is called the $k$-th order differencing operator. 
    \[\Delta^k x_t = {(1-L)}^k x_t\]
\end{itemize}

\section{Autoregressive Processes}

\begin{itemize}
    \item Time series models with correlation can be constructed from white noise.
    \item The simplest correlated stationary processess are \textit{autoregressive processeses},
    where $\{x_t\}$ is modeled as a weighted average of past observations plus a white noise 
    ``error.''
    \item The term \textit{autoregression} refers to the regression of the process on its own 
    past values. 
    \item The ACF of AR processes declines geometrically.
\end{itemize}

\subsection{AR(1) Models}
\begin{itemize}
    \item Let $\epsilon_1, \epsilon_2, \ldots$ be $\text{WN}(0, \sigma_{\epsilon}^{2})$. Then 
    $\{x_t\}$ is an $\text{AR}(1)$ process if, for some constant parameter $\phi$
    \[ x_t = \phi x_{t-1} + \epsilon_t \]
    for all $t$.
    \item $\phi x_{t-1}$ may be thought of as representing the memory of the past observation
    into the present value of the process. This is what we believe we can model and predict.
    \item $\phi$ determines the amount of feedback, where a larger absolute value results in 
    more feedback.
    \item $\epsilon_t$ represents the effect of new information that cannot be modeled, hence,
    it is represented as white noise. This is what we cannot model nor predict. 
    \item Properties of a Stationary AR(1) Process 
    \begin{itemize}
        \item $E(x_t) = \mu$ 
        \item $\text{Var}(x_t)= \gamma_0 = \sigma_x^2 = \frac{\sigma_{\epsilon_t}^{2}}{1-
        \phi^2}$
    \end{itemize}
    \item The ACF of an AR(1) process depends only on one parameter, $\phi$. This parsimony 
    comes at the cost that the ACF has only a very limited range of shapes. If the ACF does not
    behave in one of these shapes, the AR(1) model is not suitable.
    \begin{figure}[H] 
        \centering 
        \includegraphics[width=5in]{imgs/ar1_acf.png}
        \caption{ACF of AR(1) processes with $\phi$ equal to 0.95, 0.75, 0.2, and -0.9.}
    \end{figure}
    \item When $|\phi| < 1$ the process is stationary.
    \item When $\phi=1$ the process is not stationary and takes the form of a random walk.
    Recall a random walk, in each period, takes a step random step that is i.i.d. from its 
    previous steps. 
    \item When $|\phi|>1$ the process is non-stationary and has explosive behavior.
    \item A non-zero value of $\phi$ mean that there is some information in the previous
    observation, but a small value of $\phi$ means the prediction will not be very accurate.
\end{itemize}

\subsection{AR($p$) Models}
\begin{itemize}
    \item A more flexible adaptation of an AR model that is still parsimonious and regresses on
    the $p$ past values.
    \item Let $\epsilon_1, \epsilon_2, \ldots$ be $\text{WN}(0, \sigma_{\epsilon}^{2})$. Then 
    $\{x_t\}$ is an $\text{AR}(p)$ process if, for constant parameters $\phi_1, \phi_2, \hdots,
    \phi_{t-p}$
    \[x_t = \phi_1 x_{t-1} + \phi_2 x_{t-2} + \hdots + \phi_p x_{t-p} + \epsilon_t\]
    \item If the $\text{AR}(p)$ model fits the time series well, then the residuals should look 
    like white noise. The residual autocorrelation can be detected examining the sample ACF of
    the residuals and using the Ljung-Box test. Any significant residual autocorrelation is a sign 
    the $\text{AR}(p)$ model does not fit well.
    \item A problem with AR models is that they often need a rather large value of $p$ to fit 
    a dataset.
    %%%%%% YULE WALKER?
\end{itemize}

%\subsection{Determining Lag Length for AR Processes}

\section{Moving Average Processes}

\begin{itemize}
    \item With AR models, feeding past values into the current value has the effect of having 
    at least some correlation at all lags. Sometimes data show correlation only at short lags, 
    in these cases a MA process may be a suitable alternative.
    \item A process $\{x_t\}$ is a \textit{moving average process} if $\{x_t\}$ can be 
    expressed as a weighted average (moving average) of the past values of the white noise 
    process $\{\epsilon_t\}$.
\end{itemize}

\subsection{MA($1$)}
\begin{itemize}
    \item The $\text{MA}(1)$ (moving average of order 1) process is 
    \[x_t = \epsilon_t + \theta \epsilon_{t-1}\]
    where, as before, the $\epsilon_t$ are weak $\text{WN}(0, \sigma_{\epsilon}^{2})$.
\end{itemize}

\subsection{MA($q$)}
\begin{itemize}
    \item The $\text{MA}(q)$ (moving average of order $q$) process is 
    \[x_t = \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \hdots + \theta_q \epsilon_{t-q} \]
    where, as before, the $\epsilon_t$ are weak $\text{WN}(0, \sigma_{\epsilon}^{2})$.
\end{itemize}

\section{ARMA Processes}
\begin{itemize}
    \item Stationary time series with complex autocorrelation behavior are often more 
    parsimoniously modeled by mixed AR and MA (ARMA) processes rather than by a pure AR or pure
    MA process.
\end{itemize}

\subsection{ARMA($p,q$)}
\begin{itemize}
    \item An $\text{ARMA}(p,q)$ model combines both AR and MA terms, and is defined by the 
    equation 
    \[x_t = \phi_1 x_{t-1} + \hdots + \phi_p x_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + 
    \hdots + \theta_q \epsilon_{t-q}\]
    which shows how $\{x_t\}$ depends on lagged values of itself and lagged values of the white
    noise process.
\end{itemize}

\section{ARIMA Processes}
\begin{itemize}
    \item Often the first or perhaps second differences of nonstationarity time series are 
    stationary. \textit{Autoregressive integrated moving average} (ARIMA) processed include 
    stationary as well as nonstationary processes.
    \item A time series $\{x_t\}$ is said to be an $\text{ARIMA}(p,d,q)$ process if $\Delta^d 
    x_t$ is $\text{ARMA}(p,q)$.
    \item An $\text{ARIMA}(p,d,q)$ is stationary if $d=0$, otherwise its difference of order 
    $d$ or above are stationary. 
    \item An $\text{ARIMA}(p,0,q)$ is the same as an $\text{ARMA}(p,q)$.
    \item A process is $I(d)$ if it is stationary after being differenced $d$ times.
    \item An $\text{ARIMA}(p,d,q)$ process has $d$ unit roots, therefore we want to difference 
    the process $d$ times to get rid of the unit roots. 
\end{itemize}

\section{Impulse Response Functions}
\begin{itemize}
    \item The idea of an IRF is to enact a single shock to $\epsilon_t$ (in period $t$) and,
    via the IRF, see how the shock affects $x_{t+1}, x_{t+2}, \ldots$
    \item It allows us to start thinking about causes and effects.
    \item If the impulse response sizable for even long horizons indicates possible 
    nonstationarity. 
    \item For an AR(1) process, notice that the effect of the shock never dies when $\phi=1$,
    and it dies out quicker and quicker as we move from $\phi=0.95$ to $\phi=0.5$. This makes 
    sense as the closer to 1 $|\phi|$ is, the more useful information there is in the previous 
    observation in forecasting the current observation.
    \begin{figure}[H] 
        \centering 
        \includegraphics[width=4in]{imgs/irf.png}
        \caption{IRF on an AR(1) Process}
    \end{figure}
\end{itemize}

\section{Unit Root Tests}
\begin{itemize}
    \item Determining whether a time series is best modeled as stationary or nonstationary can
    be difficult, unit root tests aid in this process. 
    \item Recall the definition of an $\text{ARMA}(p,q)$ process
    \[x_t = \phi_1 x_{t-1} + \hdots + \phi_p x_{t-p} + \epsilon_t + \theta_1 \epsilon_{t-1} + 
    \hdots + \theta_q \epsilon_{t-q}\]
    \item The condition of $\{x_t\}$ to be stationary is that all roots of the polynomial 
    \[1 - \phi_1 x - \hdots - \phi_p x^p\] 
    have absolute values greater than one.
    \item If there is a unit root, that is, a root with an absolute value equal to 1, then the 
    ARMA process is nonstationary and behaves much like a random walk. This is called the unit 
    root case. 
    \item Unit root tests are used to decide if an AR model have an absolute root equal to 1.
    \item We can always look at a time series and say we think it is or isn't stationary, but 
    the Dickey-Fuller and Augmented Dickey-Fuller tests give us a robust way to do so. 
    \item A popular unit root test is the augmented Dickey-Fuller test (ADF test). In this test
    \begin{itemize}
        \item $H_0$: there is a unit root (the process is nonstationary)
        \item $H_1$: the process is stationary
    \end{itemize} 
\end{itemize}

\subsection{Dickey-Fuller Test}
\begin{itemize}
    \item Assumes our time series is an $AR(1)$ model, that is 
    \[ Y_t = \phi Y_{t-1} + \epsilon_t \]
    \item $H_0$: there is a unit root, that is, $\phi = 1$ (the process is nonstationary)
    \item $H_1$: the process is stationary, that is $|\phi|<1$
    \item Let $\pi = (\phi - 1)$. The Dickey-Fuller test rewrites the $AR(1)$ model as follows
    to make the left hand side stationary.
    \begin{align*}
        Y_t &= \phi Y_{t-1} + \epsilon_t \\
        Y_t - Y_{t-1} &= \phi Y_{t-1} - Y_{t-1} + \epsilon_t \\
        \Delta Y_t &= (\phi - 1) Y_{t-1} + \epsilon_t \\
        \Delta Y_t &= \pi Y_{t-1} + \epsilon_t \\
    \end{align*}
    \item We now rewrite the hypotheses in terms of $\pi$, that is 
    \begin{itemize}
        \item $H_0$: $\pi = 0$
        \item $H_1$: $\pi < 0$
    \end{itemize}
\end{itemize}

\subsection{Augmented Dickey-Fuller Test}
\begin{itemize}
    \item Assumes our time series is an $AR(p)$ model, that is 
    \[Y_t = \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \hdots + \phi_p Y_{t-p} + \epsilon_t\]
    \item $H_0$: there is a unit root, that is, $\phi = 1$ (the process is nonstationary)
    \item $H_1$: the process is stationary, that is $|\phi|<1$
    \item Let $\pi = (\phi - 1)$. The Dickey-Fuller test rewrites the $AR(p)$ model as follows
    to make the left hand side stationary.
    \begin{align*}
        Y_t &= \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \hdots + \phi_p Y_{t-p} + \epsilon_t \\
        \Delta Y_t &= \beta_0 + \beta_1 t + \pi Y_{t-1} + \sum_{j=i}^{p} \gamma_j \Delta Y_{t-j} + \epsilon_t \\
    \end{align*}
    \item We now rewrite the hypotheses in terms of $\pi$, that is 
    \begin{itemize}
        \item $H_0$: $\pi = 0$
        \item $H_1$: $\pi < 0$
    \end{itemize}
\end{itemize}

\section{Regression}

\section{Estimation for AR Models}
\begin{itemize}
    \item AR models can be estimated by OLS. Recall OLS is defined
    \[y = X\beta + e\]
    where 
    \[
    y = \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_n
    \end{bmatrix} \in \mathbb{R}^{n \times 1}, \,
    X = \begin{bmatrix}
        x_1' \\
        x_2' \\
        \vdots \\
        x_n'
    \end{bmatrix} \in \mathbb{R}^{n \times k}, \, 
    e = \begin{bmatrix}
        e_1 \\
        e_2 \\
        \vdots \\
        e_n
    \end{bmatrix} \in \mathbb{R}^{n \times 1}
    \]
    where $k$ is the number of regressors.   

    \item Since true errors are not observed, MA models (which depend upon the true errors), MA 
    models are not estimated by OLS. 
\end{itemize}

\subsection{OLS for AR($p$) Process}
\begin{itemize}
    \item Recall the definition of an $\text{AR}(p)$ process 
    \[z_t = \alpha + \phi_1 z_{t-1} + \phi_2 z_{t-2} + \hdots + \phi_p z_{t-p} + \epsilon_t , 
    \; t = 1, \ldots, T\]
    \item We let 
    \[y_i = z_t\]
    \[x_i= [1, z_{t-1}, \ldots, z_{t-p}]'\]
    \[\beta = [\alpha, \phi_1, \ldots, \phi_p]'\]

    where 

    \[
    y = \begin{bmatrix}
        z_{p+1} \\
        z_{p+2} \\
        \vdots \\
        z_T
    \end{bmatrix}, \,
    X = \begin{bmatrix}
        1 & z_p & \hdots & z_1 \\
        1 & z_{p+1} & \hdots & z_2 \\
        \vdots & \vdots & \ddots & \vdots \\
        1 & z_{T-1} & \hdots & z_{T-p}
    \end{bmatrix}, \, 
    e = \begin{bmatrix}
        e_1 \\
        e_2 \\
        \vdots \\
        e_n
    \end{bmatrix}
    \]
\end{itemize}

\subsection{Autocorrelated Residuals}
\begin{itemize}
    \item If the residuals are autocorrelated, the regression is misspecified. 
\end{itemize}
\section{Spurious Regression}
\begin{itemize}
    \item This is concluding there is a relationship between varables when there is none. 
    \item The regression is spurious when we regress one random walk onto another independent 
    random walk. It is spurious as the regression will most likely indicate a non-existent 
    relationship. 
    \item Rule of thumb: If we have an $R^2$ greater than our Durbin Watson statistic, our 
    relationship is likely spurious. 
    \item Always check the stationarity of the residual. The gression is spurious if the 
    residual is nonstationary (cannot reject the null of the unit root test).
    \item Just because two series move together does not mean they are related. 
\end{itemize}

\section{Estimation}
\subsection{Estimation Basics}
\begin{itemize}
    \item An estimator is any procedure or formula that is used to predict or estimate the
    true value of some unknown quantity by using information contained in data points of a 
    sample.
\end{itemize}

\subsection{Properties of Estimation}
\begin{itemize}
    \item Consistency: A consistent estimator is one which produces a better and better 
    estimate as the sample size of the data increases. 
    \item Bias: The bias of an estimator is the difference between estimator's expected value 
    and the true value of the parameter being estimated. 
\end{itemize}

\subsection{Ordinary Least Squares}
\begin{itemize}
    \item A method of parameter estimation to determine the line of best fit to the data, where 
    best fit is defined as the minimization sum of squared residuals.
    \item Least Squares Estimator
    \[\hat{\beta} = {(\boldsymbol{X}^T \boldsymbol{X})}^{-1} \boldsymbol{X}^T \boldsymbol{Y}\]
    \item Assumptions
    \begin{enumerate}
        \item The regression model is linear in parameters. 
        \item The samples are randomly sampled. 
        \item The errors and independent variables are independent (errors are orthogonal to regressors).
        \item There is no multi-collinearity (there is no linear relationship between 
        independent variables).
        \item The variance of the errors of the regression model is constant (the errors are 
        homoskedastic). $D = \sigma^2 \boldsymbol{I}$
        \item The errors are not correlated with each other or themselves. 
        \item The errors are normally distributed.
    \end{enumerate}
\end{itemize}

\subsection{Generalized Least Squares}
\begin{itemize}
    \item A generalization of the OLS estimation technique. 
    \item It is suitable for fitting linear models on data that exhibit heteroskedasticity 
    (non-constant variance) and/or autocorrelation, which are common properties in real world 
    data. OLS assumes that these properties are not present, therefore making it unfit for 
    estimation in this case. 
    \item When errors are i.i.d. ($\Sigma = \sigma^2 \boldsymbol{I}$), OLS is appropriate, but
    when errors are not i.i.d. ($\Sigma \ne \sigma^2 \boldsymbol{I}$), OLS is not appropriate.
    \item Generalized Least Squares Estimator
    \[\hat{\beta} = {(\boldsymbol{X}^T \Sigma^{-1} \boldsymbol{X})}^{-1} \boldsymbol{X}^T 
    \Sigma^{-1} \boldsymbol{Y}\]
    \item A special case of GLS is Weighted Least Squares, which occurs when $\Sigma$ is a 
    diagonal matrix. The intution is observations are weighted according to their volatilities
    and are estimated in the same manner as regular GLS, that is 
    \[\hat{\beta} = {(\boldsymbol{X}^T \Sigma^{-1} \boldsymbol{X})}^{-1} \boldsymbol{X}^T 
    \Sigma^{-1} \boldsymbol{Y}\]
    \item GLS is theoretically more efficient than OLS, but hard to implement in practice. 
    \item Drawbacks
    \begin{enumerate}
        \item $\Sigma$ is usually unknown and has to be estimated.
    \end{enumerate}
\end{itemize}


\subsection{Heteroskedasticity and Autocorrelation Consistent (HAC) Standard Errors}
\begin{itemize}
    \item These are methods to esimate consistent standard errors when errors exhibit 
    heteroskedasticity and/or autocorrelation.
    \item White Heteroskasticity Consistent (HC) Covariance Estimatior
    \[
    \hat{\text{Cov}}_{HC} (\hat{\beta} | \boldsymbol{x}_1, \ldots, \boldsymbol{x}_n) = 
    {(\boldsymbol{X}^T \boldsymbol{X})}^{-1} \hat{\boldsymbol{C}}_{HC} {(\boldsymbol{X}^T 
    \boldsymbol{X})}^{-1}
    \]
    where 
    \[
    \hat{\boldsymbol{C}}_{HC} = \boldsymbol{X}^T \hat{\Sigma}_{\hat{\boldsymbol{\epsilon}}}
    \boldsymbol{X}
    \]
    \item Newey West Heteroskedasticity and Autocorrelation Consistent (HAC)
    % \[
    %     \hat{\text{Cov}}_{HC} (\hat{\beta} | \boldsymbol{x}_1, \ldots, \boldsymbol{x}_n) = 
    %     {(\boldsymbol{X}^T \boldsymbol{X})}^{-1} \hat{\boldsymbol{C}}_{HAC} {(\boldsymbol{X}^T 
    %     \boldsymbol{X})}^{-1}
    %     \]
    %     where 
    %     \[
    %     \hat{\boldsymbol{C}}_{HAC} = \hat{\boldsymbol{C}}_{HC}
    %     \boldsymbol{X}^T \hat{\Sigma}_{\hat{\boldsymbol{\epsilon}}}
    %     \boldsymbol{X}
    %     \]
\end{itemize}

\subsection{Maximum Likelihood}
\begin{itemize}
    \item Likelihood Function: Let $X_1, \ldots, X_n$ be an i.i.d. random sample from a 
    distribution with a parameter $\theta$ ($\theta$ may be a real value or a vector, $\theta =
    (\theta_1, \ldots, \theta_k)$). Suppose $x_1, \ldots, x_n$ are observed values of $X_1, 
    \ldots, X_n$. If the $X_i$'s are discrete r.v.s, the likelihood function is defined as the 
    probability of observing the sample in the real world as a function of $\theta$

    \[L(x_1, \ldots, x_n | \theta) = P(X_1 = x_1, \ldots, X_n = x_n | \theta) = \prod_{x=1}^{n} 
    P(x_i | \theta) \]

    If the $X_i$'s are continuous r.v.s, we have a similar likelihood function
    \[L(x_1, \ldots, x_n | \theta) = f(X_1 = x_1, \ldots, X_n = x_n | \theta) = \prod_{x=1}^{n} 
    f(x_i | \theta) \]

    \item As sums are often easier to deal with than products, we modify this to the log 
    likelihood function.  If the $X_i$'s are discrete r.v.s, this is defined as

    \[ \log{L(x_1, \ldots, x_n | \theta)} = \sum_{x=1}^{n} \log{P(x_i | \theta)}\]
    
    If the $X_i$'s are continuous r.v.s, we have a similar log likelihood function
    \[ \log{L(x_1, \ldots, x_n | \theta)} = \sum_{x=1}^{n} \log{f(x_i | \theta)} \]
    
    \item Naturally, this is a value we would like to maximize, leading us to maximum 
    log likelihood estimation.
    \[ \hat{\theta}_{mle} = \max_{\theta}{\log{L(x | \theta)}}\]
\end{itemize}

\subsection{MLE with Autocorrelation}
\begin{itemize}
    \item Since the sample is no longer i.i.d, the density is no longer the product of the 
    individual densities. Instead, we use conditional densities 
    \[f(x_1, \ldots, x_n) = f(x_n | x_{n-1}, \ldots, x_1) f(x_{n-1} | x_{n-2}, \ldots, x_1)
    \ldots f(x_2 | x_1) f(x_1)\]
\end{itemize}


\section{Multivariate Time Series}
\begin{itemize}
    \item Multivariate regression is not multiple regression.
    \item Multivariate regression refers to modeling multiple target variables at the same time. 
\end{itemize}

\subsection{Vector Autoregressions (VAR)}
\begin{itemize}
    \item A generalization of AR processes for multivariate series. 
\end{itemize}


\section{Cointegration}
\subsection{Cointegration with Two Time Series}
\begin{itemize}
    \item Up until now, if we've had non-stationary time series, it has been a bad idea to 
    regress them on one another. 
    \item Sometimes we can find 2 or more non-stationary time series that are so closely 
    connected that a linear combination of them is stationary. That is, they have a common 
    trend, the linear combination gets rid of the trend, and the result is a stationary time 
    series which is just the deviation from the trend. 
    \item In these cases, we can create a cointegrating vector, which is the vector of the 
    coefficients of this linear combination. 
    \item Slightly more formally, 2 times series, $ \boldsymbol{Y}_t = (Y_{1,t}, Y_{2,t})'$ 
    are cointegrated if each is $I(1)$ but there exists a cointegration vector, $\alpha = 
    (1, -\alpha)$, such that $ \alpha'\boldsymbol{Y}_t = Y_{1,t} - \alpha Y_{2,t}$ is 
    stationary. That is, $\alpha'\boldsymbol{Y}_t \sim I(0)$.
    \item To test for cointegration, regress $Y_{1,t}$ on $Y_{2,t}$ to get an $\alpha$ and then 
    we can check the residuals to see if they are stationary. % Partly true.
    \item The stationary cointegration error is the deviation from the trend as is defined 
    \[Z_t =  Y_{1,t} - \alpha Y_{2,t} \]
\end{itemize}




%%%
\section{Misc.}
\begin{itemize}
    \item Returns are closer to i.i.d. than prices and overall exhibit more attractive 
    statistical qualities than prices, therefore making it more sensible to study returns 
    rather than prices. 
\end{itemize}

\section{Definitions}
\begin{itemize}
    \item Homoskedasticity: A condition in which the variance of the residual is constant, that
    is, the error term does not vary much. In other words, the variance of the data points is 
    roughly the same for all data points. 
\end{itemize}

\end{document}
